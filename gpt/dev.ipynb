{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tinyshakespeare.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Unique characters in the text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# Mapping from characters to integers and vice versa\n",
    "char_to_int = {c: i for i, c in enumerate(chars)}\n",
    "int_to_char = {i: c for i, c in enumerate(chars)}\n",
    "\n",
    "encode = lambda x: [char_to_int[c] for c in x] # x: str -> list[int]\n",
    "decode = lambda x: ''.join([int_to_char[i] for i in x]) # x: list[int] -> str\n",
    "\n",
    "print(encode('Hello World!'))\n",
    "print(decode(encode('Hello World!')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and validation splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "split = int(len(data) * 0.9) # 90% train, 10% val\n",
    "train_data, val_data = data[:split], data[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are a total of block_size training examples in each block\n",
    "block_size = 8\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    print(f'context: {x[:t+1].tolist()} -> target: {y[t]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training with these different lengths of contexts from a size of 1 to block_size is important to ensure the transformer learns to deal with different context lengths. This is useful during inference because the model can generate text from as little as one character of context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4 # Number of sequences to process in parallel\n",
    "block_size = 8 # Maximum context length for predictions\n",
    "\n",
    "def get_batch(split: str) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Generate a random batch of context and target sequences.\"\"\"\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    # Randomly sample batch_size number of starting indices\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "# Get a batch of context and target sequences\n",
    "xb, yb = get_batch('train')\n",
    "print(f'xb: {xb}\\nyb: {yb}')\n",
    "\n",
    "# xb and yb are both tensors of shape (batch_size, block_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the context and target sequences for each batch element\n",
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        print(f'context: {xb[b, :t+1].tolist()} -> target: {yb[b, t]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B - batch size, T - block size (time step), C - embedding dimension (vocab size)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embed_table = nn.Embedding(vocab_size, vocab_size) # (B,T) -> (B,T,C)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, y: torch.Tensor = None) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        logits = self.token_embed_table(x)\n",
    "\n",
    "        if y is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            # Flatten batch and sequence dimensions to use F.cross_entropy\n",
    "            logits = logits.view(B*T, C)\n",
    "            y = y.view(B*T)\n",
    "            loss = F.cross_entropy(logits, y)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, x: torch.Tensor, max_tokens: int) -> torch.Tensor:\n",
    "        for _ in range(max_tokens):\n",
    "            # Get the previous predictions\n",
    "            logits, _ = self(x)\n",
    "            # Keep only the last prediction\n",
    "            logits = logits[:, -1, :] # (B,C)\n",
    "            # Apply softmax to convert logits into probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B,C)\n",
    "            # Sample from the probability distribution\n",
    "            x_next = torch.multinomial(probs, num_samples=1) # (B,1)\n",
    "            # Concatenate the new prediction to the previous context\n",
    "            x = torch.cat([x, x_next], dim=1) # (B,T+1)\n",
    "        return x\n",
    "    \n",
    "model = BigramLanguageModel()\n",
    "\n",
    "total_params = sum(param.numel() for param in model.parameters())\n",
    "print(f'Model parameters: {total_params}')\n",
    "\n",
    "# Generate\n",
    "context = torch.zeros((1, 1), dtype=torch.long)\n",
    "print(decode(model.generate(context, max_tokens=32)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The integer associated with each character is used as an index to look up the corresponding row in the embedding table. This row is a trainable vector (of size `n_embed`) representation of the character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 32 # Sequences to process in parallel\n",
    "max_iters = 2500 # Iterations to train the model\n",
    "lr = 1e-2 # Learning rate\n",
    "\n",
    "# Training the model\n",
    "optimiser = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "loss = torch.tensor(torch.inf)\n",
    "\n",
    "for i in range(max_iters):\n",
    "\n",
    "    if i % (max_iters // 10) == 0 or i == max_iters - 1:\n",
    "        print(f'iteration {i}, loss: {loss.item()}')\n",
    "\n",
    "    # Get a batch of context and target sequences\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # Compute the gradients and update the weights\n",
    "    _, loss = model(xb, yb) # Forward pass\n",
    "    optimiser.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimiser.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate\n",
    "context = torch.zeros((1, 1), dtype=torch.long)\n",
    "print(decode(model.generate(context, max_tokens=32)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B, T, C = 4, 8, 32\n",
    "x = torch.randn(B, T ,C)\n",
    "\n",
    "# Bag of words. Calculate x[b,t] = mean_{t'<=t} x[b,t']\n",
    "xbow_1 = torch.zeros((B, T, C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xbow_1[b, t] = torch.mean(x[b, :t+1], 0)\n",
    "\n",
    "# Version 2. Parallelised. W is a lower triangular matrix which can be used for weighted aggregation\n",
    "W = torch.tril(torch.ones(T, T))\n",
    "W = W / W.sum(1, keepdim=True)\n",
    "xbow_2 = W @ x\n",
    "\n",
    "# Version 3. Parallelised. Uses softmax. W represents the same lower triangular matrix as before\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "W = torch.zeros((T, T))\n",
    "W = W.masked_fill(tril == 0, float('-inf'))\n",
    "W = F.softmax(W, dim=-1)\n",
    "xbow_3 = W @ x\n",
    "\n",
    "# Check that the three methods are equivalent\n",
    "torch.allclose(xbow_1, xbow_2) and torch.allclose(xbow_1, xbow_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B - batch size, T - block size (time step), C - embedding dimension, H - head size\n",
    "\n",
    "# Single head self-attention\n",
    "head_size = 16\n",
    "n_embed = 32\n",
    "key = nn.Linear(n_embed, head_size, bias=False) # (B,T,C) -> (B,T,H)\n",
    "query = nn.Linear(n_embed, head_size, bias=False) # (B,T,C) -> (B,T,H)\n",
    "value = nn.Linear(n_embed, head_size, bias=False) # (B,T,C) -> (B,T,H)\n",
    "k = key(x)\n",
    "q = query(x)\n",
    "\n",
    "# Compute the scaled dot-product attention\n",
    "W = q @ k.transpose(-2, -1) # (B,T,H) @ (B,H,T) -> (B,T,T)\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "W = W.masked_fill(tril == 0, float('-inf'))\n",
    "W = F.softmax(W, dim=-1)\n",
    "v = value(x)\n",
    "out = W @ v # (B,T,T) @ (B,T,H) -> (B,T,H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes:**\n",
    "- Attention is a communication mechanism. It can be viewed as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
    "- In the attention layer of a Transformer, every token is attending to a finite list of tokens previously in the sequence. This is called causal self-attention.\n",
    "- There is no notion of space. Attention simply acts over a set of vectors. This is why tokens need to be positionally encoded.\n",
    "- Each example across batch dimensions are treated independently and never interact with each other.\n",
    "- In an encoder attention block just delete the single line that performs masking with `tril`, allowing all tokens to communicate with each other and not just the previous ones. The block implemented above is called a decoder attention block because it has triangular masking and is used in autoregressive settings like language modelling.\n",
    "- 'Self-attention' just means that the keys and the values are produced from the same source as the queries (`x` in this case). In 'cross attention', the queries still get produced from `x`, but the keys and values come from a different source (such as an encoder module).\n",
    "- 'Scaled' attention additionally divides `W` by $1/\\sqrt{H}$. This ensures that when the input `Q` and `K` are of unit variance, `W` has unit variance as well and softmax will stay diffuse and not saturate (see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = torch.randn(B, T, head_size)\n",
    "q = torch.randn(B, T, head_size)\n",
    "W = q @ k.transpose(-2, -1) * head_size**-0.5\n",
    "\n",
    "k.var(), q.var(), W.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1))\n",
    "# With larger values the probabilities become more concentrated, converges to a one-hot vector\n",
    "print(torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]) * 10, dim=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT Tokeniser Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A token is a sequence of characters in a text that serves as a unit. Furthermore, tokenisation is the process of converting a text into a sequence of tokens. Tokenisation is critical to the correct functioning of transformers and bad tokenisation can cause issues with the models performance irrespective of the model architecture. If tokenisation is not done correctly, transformer models can struggle to spell words, struggle with non-English words, struggle with simple arithmetic, and even produces unintended outputs (see [SolidGoldMagikarp](https://www.lesswrong.com/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation)).\n",
    "\n",
    "Recall the GPT model in `gpt.py`:\n",
    "```Python\n",
    "class GPTLanguageModel(nn.Module):\n",
    "    \"\"\"GPT Decoder model. Consists of an embedding layer, transformer blocks, and a linear head.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embed_table = nn.Embedding(vocab_size, n_embed) # (B,T) -> (B,T,C)\n",
    "        # etc.\n",
    "```\n",
    "Tokens are the fundamental 'atoms' at the input of transformers. Each token (character) is used as an index to look up the corresponding row in the embedding table, where this row is a trainable vector (of size `n_embed`) representation of the token. Using characters as tokens is a naive approach due to the transformers having a limited context window (1024 tokens for GPT-2) in which tokens can attend to each other. Chunk vocabularies are used to tokenise text into character chunks instead of individual characters. These chunk vocabularies are constructed using the Byte Pair Encoding (BPE) algorithm (popularised in the [GPT-2 paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)). Using character chunks as tokens allows the model to attend to a wider portion of the text, which can improve performance.\n",
    "\n",
    "Note that the tokeniser is completely separate from the transformer model. It has a separate training dataset to train the vocabulary on the BPE algorithm. The tokeniser then encodes/decodes between text and sequences of tokens. The transformer model only sees the tokens and never directly deals with any text.\n",
    "\n",
    "[Tiktokeniser](https://tiktokenizer.vercel.app/) provides a visualisation into differences between various tokenisers available for GPT models. Use 'gpt2' and 'cl100k_base' as the model names to compare the tokenisation of GPT-2 and GPT-4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Hello 你好'\n",
    "\n",
    "print([ord(x) for x in text]) # Encoding to unicode values\n",
    "print(list(text.encode('utf-8'))) # Encoding to utf-8 bytes\n",
    "\n",
    "# The utf-8 encoding is different from the unicode values for non-ASCII characters \n",
    "# as it uses a variable number of bytes. 20320 is encoded as 228 189 160 in utf-8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consecutive_pairs(ints: list[int]) -> dict[tuple[int, int], int]:\n",
    "    \"\"\"\n",
    "    Generate a dictionary of the frequencies of consecutive integers in the list.\n",
    "    Example: [1, 2, 3, 1, 2] -> {(1, 2): 2, (2, 3): 1, (3, 1): 1}\n",
    "    \"\"\"\n",
    "    freq = {}\n",
    "    for pair in zip(ints, ints[1:]):\n",
    "        freq[pair] = freq.get(pair, 0) + 1\n",
    "    return freq\n",
    "\n",
    "text = 'abcab'\n",
    "tokens = list(text.encode('utf-8'))\n",
    "\n",
    "freq_pairs = consecutive_pairs(tokens)\n",
    "print(freq_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_pair(ints: list[int], pair: tuple[int, int], new_int: int) -> list[int]:\n",
    "    \"\"\"\n",
    "    Replace all consecutive occurrences of a pair of integers in the list with a new integer.\n",
    "    Example: ints=[1, 2, 3, 1, 2], pair=(1, 2), new_int=4 -> [4, 3, 4]\n",
    "    \"\"\"\n",
    "    new_ints = []\n",
    "    i = 0\n",
    "    while i < len(ints):\n",
    "        # If not at the last position AND the pair matches, replace it\n",
    "        if (i < len(ints) - 1) and ints[i:i+2] == list(pair):\n",
    "            new_ints.append(new_int)\n",
    "            i += 2\n",
    "        else:\n",
    "            new_ints.append(ints[i])\n",
    "            i += 1\n",
    "    return new_ints\n",
    "\n",
    "# Replace the most frequent pair with a new token (256)\n",
    "max_pair = max(freq_pairs, key=freq_pairs.get)\n",
    "new_tokens = replace_pair(tokens, pair=max_pair, new_int=256)\n",
    "print(f'{tokens} -> {new_tokens}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training via Byte Pair Encoding (BPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load new text from a file.\n",
    "with open('tinyshakespeare.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "\n",
    "vocab_size = 265 # Desired vocabulary size\n",
    "\n",
    "assert vocab_size >= 256\n",
    "n_merges = vocab_size - 256\n",
    "tokens = list(text.encode('utf-8'))\n",
    "merges = {} # Dictionary to store the merges\n",
    "vocab = {i: bytes([i]) for i in range(256)}\n",
    "\n",
    "# Merge the most frequent pair n_merges times to create new tokens\n",
    "for i in range(n_merges):\n",
    "    # Find the most frequent consecutive pair of tokens\n",
    "    freq_pairs = consecutive_pairs(tokens)\n",
    "    max_pair = max(freq_pairs, key=freq_pairs.get)\n",
    "    # Create a new token and assign it to an unused integer\n",
    "    new_token = 256 + i\n",
    "    tokens = replace_pair(tokens, max_pair, new_token)\n",
    "    # Store the merge and the new token in the vocab\n",
    "    merges[max_pair] = new_token\n",
    "    vocab[new_token] = vocab[max_pair[0]] + vocab[max_pair[1]]\n",
    "    print(f'{i+1}/{n_merges}: {max_pair} -> {new_token}')\n",
    "\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The text is now represented by fewer tokens\n",
    "print('New token length:', len(tokens))\n",
    "print(f'Compression ratio: {len(list(text.encode(\"utf-8\"))) / len(tokens):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding and Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(tokens: list[int]) -> str:\n",
    "    \"\"\"Decode a sequence of tokens into a string.\"\"\"\n",
    "    bytes_ = b''.join(vocab[token] for token in tokens)\n",
    "    text = bytes_.decode('utf-8', errors='replace') # Replace unknown characters\n",
    "    return text\n",
    "\n",
    "# Not every byte sequence is valid utf-8. Replacing unknown characters with '?'\n",
    "# helps to avoid decoding errors as the language model may generate tokens that\n",
    "# are not valid utf-8. For example 128 is not a valid utf-8 byte.\n",
    "print(decode([128]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text: str) -> list[int]:\n",
    "    \"\"\"Encode a string into a sequence of tokens.\"\"\"\n",
    "    tokens = list(text.encode('utf-8'))\n",
    "    while len(tokens) > 1:\n",
    "        freq_pairs = consecutive_pairs(tokens)\n",
    "        # Find the most frequent consecutive pair that has been merged\n",
    "        most_freq = min(freq_pairs, key=lambda pair: merges.get(pair, float('inf')))\n",
    "        if most_freq not in merges:\n",
    "            break # No more merges to apply\n",
    "        # Merge the pair into a new token\n",
    "        new_token = merges[most_freq]\n",
    "        tokens = replace_pair(tokens, most_freq, new_token)\n",
    "    return tokens\n",
    "\n",
    "print(encode('the quick brown fox'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the encode and decode functions are inverses\n",
    "text == decode(encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenisation Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokeniser is represented using just the learned `merges` and `vocab` variables and can encode and decode text using the BPE algorithm.\n",
    "\n",
    "**Splitting Text via RegEx Patterns (GPT-2)**\n",
    "\n",
    "Instead of directly encoding each string for tokenisation, the string is split up into a list of strings using regular expressions. All the string in this list are processed independently by the tokeniser. Therefore merges can only happen within the same string. The results are then concatenated together to form the final tokenised string. This ensures that some consecutive pairs of characters are not merged together (i.e. 'e ').\n",
    "\n",
    "The regular expression below is from the [GPT-2 tokeniser](https://github.com/openai/gpt-2/blob/master/src/encoder.py). The patterns `'s|'t|'re|'ve|'m|'ll|'d` match common contractions, however it only considers the ASCII apostrophe (') and not the unicode apostrophe (’). Furthermore, they do not ignore case and so will not match `'S|'T|'RE|'VE|'M|'LL|'D`. That is, `I'm` will be tokenised as `I`, `'m` and `I'M` will be tokenised as `I`, `'`, `M`. These are limitations of the GPT-2 tokeniser.\n",
    "\n",
    "GPT-2 also uses one special token which denotes the end of text, `<|endoftext|>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "\n",
    "regex = re.compile(r\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\")\n",
    "print(re.findall(regex, 'I\\'m. I\\'M'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vocabulary Size**\n",
    "\n",
    "Large vocabulary sizes increase the number of tokens that the model can represent. Therefore, tokens can express more information in a shorter sequence. This allows transformers to attend to more tokens in the sequence and improves the model's ability to learn long-range dependencies. However, larger vocabulary sizes mean that the embedding table is larger and hence more computationally expensive to train. Furthermore, large vocabulary sizes mean that each unique token is less likely to be seen in the training data, hence the vector representation of the token may be under-trained, leading to worse overall performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Tokeniser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from gpt.tokeniser.gpt import GPTTokeniser\n",
    "\n",
    "# Special tokens to be added to the vocabulary. GPT-4 uses these tokens\n",
    "special_tokens = {\n",
    "    '<|endoftext|>': 100257,\n",
    "    '<|fim_prefix|>': 100258,\n",
    "    '<|fim_middle|>': 100259,\n",
    "    '<|fim_suffix|>': 100260,\n",
    "    '<|endofprompt|>': 100276\n",
    "}\n",
    "\n",
    "# Load new text from a file\n",
    "with open('input.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Create a tokeniser and do 64 merges\n",
    "tokeniser = GPTTokeniser()\n",
    "vocab_size = 256 + 64\n",
    "tokeniser.train(text, vocab_size=vocab_size, verbose=True)\n",
    "\n",
    "# Register special tokens\n",
    "tokeniser.register_special_tokens(special_tokens)\n",
    "\n",
    "# Verify that the encode and decode functions are inverses\n",
    "assert text == tokeniser.decode(tokeniser.encode(text, 'all'))\n",
    "\n",
    "# Verify that save/load work as expected\n",
    "tokeniser.save('tmp')\n",
    "tokeniser = GPTTokeniser('tmp.tkn')\n",
    "\n",
    "# Verify that the encode and decode functions are inverses\n",
    "assert text == tokeniser.decode(tokeniser.encode(text, 'all'))\n",
    "\n",
    "# Remove the temporary file\n",
    "os.remove('tmp.tkn')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
