{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Unique characters in the text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# Mapping from characters to integers and vice versa\n",
    "char_to_int = {c: i for i, c in enumerate(chars)}\n",
    "int_to_char = {i: c for i, c in enumerate(chars)}\n",
    "\n",
    "encode = lambda x: [char_to_int[c] for c in x] # x: str -> list[int]\n",
    "decode = lambda x: ''.join([int_to_char[i] for i in x]) # x: list[int] -> str\n",
    "\n",
    "print(encode('Hello World!'))\n",
    "print(decode(encode('Hello World!')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and validation splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "split = int(len(data) * 0.9) # 90% train, 10% val\n",
    "train_data, val_data = data[:split], data[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are a total of block_size training examples in each block\n",
    "block_size = 8\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    print(f'context: {x[:t+1].tolist()} -> target: {y[t]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training with these different lengths of contexts from a size of 1 to block_size is important to ensure the transformer learns to deal with different context lengths. This is useful during inference because the model can generate text from as little as one character of context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4 # Number of sequences to process in parallel\n",
    "block_size = 8 # Maximum context length for predictions\n",
    "\n",
    "def get_batch(split: str) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Generate a random batch of context and target sequences.\"\"\"\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    # Randomly sample batch_size number of starting indices\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "# Get a batch of context and target sequences\n",
    "xb, yb = get_batch('train')\n",
    "print(f'xb: {xb}\\nyb: {yb}')\n",
    "\n",
    "# xb and yb are both tensors of shape (batch_size, block_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the context and target sequences for each batch element\n",
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        print(f'context: {xb[b, :t+1].tolist()} -> target: {yb[b, t]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B - batch size, T - block size (time step), C - embedding dimension (vocab size)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embed_table = nn.Embedding(vocab_size, vocab_size) # (B,T) -> (B,T,C)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, y: torch.Tensor = None) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        logits = self.token_embed_table(x)\n",
    "\n",
    "        if y is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            # Flatten batch and sequence dimensions to use F.cross_entropy\n",
    "            logits = logits.view(B*T, C)\n",
    "            y = y.view(B*T)\n",
    "            loss = F.cross_entropy(logits, y)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, x: torch.Tensor, max_tokens: int) -> torch.Tensor:\n",
    "        for _ in range(max_tokens):\n",
    "            # Get the previous predictions\n",
    "            logits, _ = self(x)\n",
    "            # Keep only the last prediction\n",
    "            logits = logits[:, -1, :] # (B,C)\n",
    "            # Apply softmax to convert logits into probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B,C)\n",
    "            # Sample from the probability distribution\n",
    "            x_next = torch.multinomial(probs, num_samples=1) # (B,1)\n",
    "            # Concatenate the new prediction to the previous context\n",
    "            x = torch.cat([x, x_next], dim=1) # (B,T+1)\n",
    "        return x\n",
    "    \n",
    "model = BigramLanguageModel()\n",
    "\n",
    "# Generate\n",
    "context = torch.zeros((1, 1), dtype=torch.long)\n",
    "print(decode(model.generate(context, max_tokens=32)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The integer associated with each character is used as an index to look up the corresponding row in the embedding table. This row is a trainable vector (of size `n_embed`) representation of the character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 32 # Sequences to process in parallel\n",
    "max_iters = 2500 # Iterations to train the model\n",
    "lr = 1e-2 # Learning rate\n",
    "\n",
    "# Training the model\n",
    "optimiser = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "loss = torch.tensor(torch.inf)\n",
    "\n",
    "for i in range(max_iters):\n",
    "\n",
    "    if i % (max_iters // 10) == 0 or i == max_iters - 1:\n",
    "        print(f'iteration {i}, loss: {loss.item()}')\n",
    "\n",
    "    # Get a batch of context and target sequences\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # Compute the gradients and update the weights\n",
    "    _, loss = model(xb, yb) # Forward pass\n",
    "    optimiser.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimiser.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate\n",
    "context = torch.zeros((1, 1), dtype=torch.long)\n",
    "print(decode(model.generate(context, max_tokens=32)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B, T, C = 4, 8, 32\n",
    "x = torch.randn(B, T ,C)\n",
    "\n",
    "# Bag of words. Calculate x[b,t] = mean_{t'<=t} x[b,t']\n",
    "xbow_1 = torch.zeros((B, T, C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xbow_1[b, t] = torch.mean(x[b, :t+1], 0)\n",
    "\n",
    "# Version 2. Parallelised. W is a lower triangular matrix which can be used for weighted aggregation\n",
    "W = torch.tril(torch.ones(T, T))\n",
    "W = W / W.sum(1, keepdim=True)\n",
    "xbow_2 = W @ x\n",
    "\n",
    "# Version 3. Parallelised. Uses softmax. W represents the same lower triangular matrix as before\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "W = torch.zeros((T, T))\n",
    "W = W.masked_fill(tril == 0, float('-inf'))\n",
    "W = F.softmax(W, dim=-1)\n",
    "xbow_3 = W @ x\n",
    "\n",
    "# Check that the three methods are equivalent\n",
    "torch.allclose(xbow_1, xbow_2) and torch.allclose(xbow_1, xbow_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B - batch size, T - block size (time step), C - embedding dimension, H - head size\n",
    "\n",
    "# Single head self-attention\n",
    "head_size = 16\n",
    "n_embed = 32\n",
    "key = nn.Linear(n_embed, head_size, bias=False) # (B,T,C) -> (B,T,H)\n",
    "query = nn.Linear(n_embed, head_size, bias=False) # (B,T,C) -> (B,T,H)\n",
    "value = nn.Linear(n_embed, head_size, bias=False) # (B,T,C) -> (B,T,H)\n",
    "k = key(x)\n",
    "q = query(x)\n",
    "\n",
    "# Compute the scaled dot-product attention\n",
    "W = q @ k.transpose(-2, -1) # (B,T,H) @ (B,H,T) -> (B,T,T)\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "W = W.masked_fill(tril == 0, float('-inf'))\n",
    "W = F.softmax(W, dim=-1)\n",
    "v = value(x)\n",
    "out = W @ v # (B,T,T) @ (B,T,H) -> (B,T,H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes:**\n",
    "- Attention is a communication mechanism. It can be viewed as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
    "- In the attention layer of a Transformer, every token is attending to a finite list of tokens previously in the sequence. This is called causal self-attention.\n",
    "- There is no notion of space. Attention simply acts over a set of vectors. This is why tokens need to be positionally encoded.\n",
    "- Each example across batch dimensions are treated independently and never interact with each other.\n",
    "- In an encoder attention block just delete the single line that performs masking with `tril`, allowing all tokens to communicate with each other and not just the previous ones. The block implemented above is called a decoder attention block because it has triangular masking and is used in autoregressive settings like language modelling.\n",
    "- 'Self-attention' just means that the keys and the values are produced from the same source as the queries (`x` in this case). In 'cross attention', the queries still get produced from `x`, but the keys and values come from a different source (such as an encoder module).\n",
    "- 'Scaled' attention additionally divides `W` by $1/\\sqrt{H}$. This ensures that when the input `Q` and `K` are of unit variance, `W` has unit variance as well and softmax will stay diffuse and not saturate (see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = torch.randn(B, T, head_size)\n",
    "q = torch.randn(B, T, head_size)\n",
    "W = q @ k.transpose(-2, -1) * head_size**-0.5\n",
    "\n",
    "k.var(), q.var(), W.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1))\n",
    "# With larger values the probabilities become more concentrated, converges to a one-hot vector\n",
    "print(torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]) * 10, dim=-1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
