{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('names.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read().split('\\n')\n",
    "\n",
    "# Unique characters in the text\n",
    "chars = ['.'] + sorted(list(set(''.join(text))))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# Mapping from characters to integers and vice versa\n",
    "char_to_int = {c: i for i, c in enumerate(chars)}\n",
    "int_to_char = {i: c for i, c in enumerate(chars)}\n",
    "\n",
    "# B - batch size, T - block size, C - embedding dimension (vocab size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 3 # Context length for predictions\n",
    "\n",
    "def build_dataset(text: list[str]) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Builds the dataset for training the model.\"\"\"\n",
    "    X, Y = [], []\n",
    "    for word in text:\n",
    "        context = [0] * block_size # Padding the context with initial '.' characters\n",
    "        for char in word + '.':\n",
    "            ix = char_to_int[char]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix] # Update context\n",
    "    return torch.tensor(X), torch.tensor(Y)\n",
    "\n",
    "random.shuffle(text)\n",
    "split = int(len(text) * 0.9) # 90% train, 10% val\n",
    "\n",
    "x_train, y_train = build_dataset(text[:split])\n",
    "x_val, y_val = build_dataset(text[split:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embd = 2 # Embedding dimension\n",
    "n_hidden = 200 # Neurons in the hidden layer\n",
    "\n",
    "C = torch.randn((vocab_size, n_embd)) # Embedding table (B,T) -> (B,T,C)\n",
    "# Hidden layer\n",
    "W_hidden = torch.randn((block_size * n_embd, n_hidden))\n",
    "b_hidden = torch.randn(n_hidden)\n",
    "# Output layer\n",
    "W_out = torch.randn((n_hidden, vocab_size))\n",
    "b_out = torch.randn(vocab_size)\n",
    "\n",
    "params = [C, W_hidden, b_hidden, W_out, b_out]\n",
    "for param in params:\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def split_loss(split: str) -> None:\n",
    "    \"\"\"Evaluate the loss over the given split.\"\"\"\n",
    "    x, y = (x_train, y_train) if split == 'train' else (x_val, y_val)\n",
    "    # Forward pass\n",
    "    emb = C[x] # Embed characters into vectors (B,T) -> (B,T,C)\n",
    "    emb = emb.view(emb.size(0), -1) # Concatenate the embeddings (B,T,C) -> (B,TC)\n",
    "    hpreact = emb @ W_hidden + b_hidden\n",
    "    h = torch.tanh(hpreact)\n",
    "    logits = h @ W_out + b_out\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    return loss.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding a Suitable Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32 # Number of samples per batch\n",
    "max_iters = 1000\n",
    "\n",
    "lrs_exp = torch.linspace(-3, 0, max_iters) # Linearly decrease the learning rate from 1e-3 to 1e-0\n",
    "lrs = 10 ** lrs_exp\n",
    "\n",
    "# Mini-batch gradient descent\n",
    "losses = []\n",
    "for i in range(max_iters):\n",
    "\n",
    "    ix = torch.randint(0, x_train.shape[0], (batch_size, )) # Mini-batches\n",
    "    xb, yb = x_train[ix], y_train[ix]\n",
    "\n",
    "    # Forward pass\n",
    "    emb = C[xb] # Embed characters into vectors (B,T) -> (B,T,C)\n",
    "    emb = emb.view(emb.size(0), -1)  # Concatenate the embeddings (B,T,C) -> (B,TC)\n",
    "    hpreact = emb @ W_hidden + b_hidden\n",
    "    h = torch.tanh(hpreact)\n",
    "    logits = h @ W_out + b_out\n",
    "\n",
    "    # Cross entropy loss\n",
    "    loss = F.cross_entropy(logits, yb)\n",
    "    losses.append(loss.data)\n",
    "\n",
    "    # Backward pass\n",
    "    for param in params:\n",
    "        param.grad = None # Set the gradient to zero\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the parameters\n",
    "    for param in params:\n",
    "        param.data -= lrs[i] * param.grad\n",
    "\n",
    "# Plot the mini-batch loss vs. learning rate\n",
    "plt.plot(lrs_exp, losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data shows the best learning rate occurs at roughly 1e-1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise the Learned Embedding Matrix $C$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-1\n",
    "max_iters = 100000\n",
    "\n",
    "# Mini-batch gradient descent\n",
    "losses = []\n",
    "for i in range(max_iters):\n",
    "\n",
    "    ix = torch.randint(0, x_train.shape[0], (batch_size, )) # Mini-batches\n",
    "    xb, yb = x_train[ix], y_train[ix]\n",
    "\n",
    "    # Forward pass\n",
    "    emb = C[xb] # Embed characters into vectors (B,T) -> (B,T,C)\n",
    "    emb = emb.view(emb.size(0), -1)  # Concatenate the embeddings (B,T,C) -> (B,TC)\n",
    "    hpreact = emb @ W_hidden + b_hidden\n",
    "    h = torch.tanh(hpreact)\n",
    "    logits = h @ W_out + b_out\n",
    "\n",
    "    # Cross entropy loss\n",
    "    loss = F.cross_entropy(logits, yb)\n",
    "    losses.append(loss.data)\n",
    "\n",
    "    # Backward pass\n",
    "    for param in params:\n",
    "        param.grad = None # Set the gradient to zero\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the parameters\n",
    "    for param in params:\n",
    "        param.data -= lr * param.grad\n",
    "\n",
    "    if i % (max_iters // 10) == 0 or i == max_iters - 1:\n",
    "        print(f'Iteration {i:2d} | Loss (Mini-Batch): {loss.data:.4f}')\n",
    "\n",
    "print(f'Train loss: {split_loss(\"train\"):.4f} | Val loss: {split_loss(\"val\"):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the embedding matrix C for all characters\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.scatter(C[:,0].data, C[:,1].data, s=200)\n",
    "for i in range(C.shape[0]):\n",
    "    plt.text(C[i,0].item(), C[i,1].item(), int_to_char[i], ha='center', va='center', color='white')\n",
    "plt.grid('minor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has learned to a basic clustering of the characters (all the vowels are clustered together with similar vector embeddings). The embedding dimension was set to 2 for visualisation purposes. From now on, a higher embedding dimension will be used to improve the performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of Initial Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefining the model with a larger embedding dimension of 10\n",
    "\n",
    "n_embd = 10 # Embedding dimension\n",
    "n_hidden = 200 # Neurons in the hidden layer\n",
    "\n",
    "C = torch.randn((vocab_size, n_embd)) # Embedding table (B,T) -> (B,T,C)\n",
    "# Hidden layer\n",
    "W_hidden = torch.randn((block_size * n_embd, n_hidden))\n",
    "b_hidden = torch.randn(n_hidden)\n",
    "# Output layer\n",
    "W_out = torch.randn((n_hidden, vocab_size))\n",
    "b_out = torch.randn(vocab_size)\n",
    "\n",
    "params = [C, W_hidden, b_hidden, W_out, b_out]\n",
    "for param in params:\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mini-batch gradient descent\n",
    "losses = []\n",
    "for i in range(max_iters):\n",
    "\n",
    "    ix = torch.randint(0, x_train.shape[0], (batch_size, )) # Mini-batches\n",
    "    xb, yb = x_train[ix], y_train[ix]\n",
    "\n",
    "    # Forward pass\n",
    "    emb = C[xb] # Embed characters into vectors (B,T) -> (B,T,C)\n",
    "    emb = emb.view(emb.size(0), -1)  # Concatenate the embeddings (B,T,C) -> (B,TC)\n",
    "    hpreact = emb @ W_hidden + b_hidden\n",
    "    h = torch.tanh(hpreact)\n",
    "    logits = h @ W_out + b_out\n",
    "\n",
    "    # Cross entropy loss\n",
    "    loss = F.cross_entropy(logits, yb)\n",
    "    losses.append(loss.data)\n",
    "\n",
    "    # Backward pass\n",
    "    for param in params:\n",
    "        param.grad = None # Set the gradient to zero\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the parameters\n",
    "    for param in params:\n",
    "        param.data -= lr * param.grad\n",
    "\n",
    "    if i % (max_iters // 10) == 0 or i == max_iters - 1:\n",
    "        print(f'Iteration {i:2d} | Loss (Mini-Batch): {loss.data:.4f}')\n",
    "\n",
    "# Plot the mini-batch loss\n",
    "plt.plot(losses)\n",
    "\n",
    "print(f'Train loss: {split_loss(\"train\"):.4f} | Val loss: {split_loss(\"val\"):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix: Large Initial Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = torch.randn((vocab_size, n_embd)) # Embedding table (B,T) -> (B,T,C)\n",
    "# Hidden layer\n",
    "W_hidden = torch.randn((block_size * n_embd, n_hidden))\n",
    "b_hidden = torch.randn(n_hidden)\n",
    "# Output layer\n",
    "W_out = torch.randn((n_hidden, vocab_size)) * 0.01 # Initialised to small values\n",
    "b_out = torch.randn(vocab_size) * 0 # Initialised to zero\n",
    "\n",
    "params = [C, W_hidden, b_hidden, W_out, b_out]\n",
    "for param in params:\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialising the weights in the output layer to be small random values ensures that the network is not confidently wrong about the output (and so has a high loss). This decreases the initial loss and allows the network to learn more effectively as it does not have to spend the first few training iterations correcting the initial large errors and can instead spend more time optimising the weights to reduce the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mini-batch gradient descent\n",
    "losses = []\n",
    "for i in range(max_iters):\n",
    "\n",
    "    ix = torch.randint(0, x_train.shape[0], (batch_size, )) # Mini-batches\n",
    "    xb, yb = x_train[ix], y_train[ix]\n",
    "\n",
    "    # Forward pass\n",
    "    emb = C[xb] # Embed characters into vectors (B,T) -> (B,T,C)\n",
    "    emb = emb.view(emb.size(0), -1)  # Concatenate the embeddings (B,T,C) -> (B,TC)\n",
    "    hpreact = emb @ W_hidden + b_hidden\n",
    "    h = torch.tanh(hpreact)\n",
    "    logits = h @ W_out + b_out\n",
    "\n",
    "    # Cross entropy loss\n",
    "    loss = F.cross_entropy(logits, yb)\n",
    "    losses.append(loss.data)\n",
    "\n",
    "    # Backward pass\n",
    "    for param in params:\n",
    "        param.grad = None # Set the gradient to zero\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the parameters\n",
    "    for param in params:\n",
    "        param.data -= lr * param.grad\n",
    "\n",
    "    if i % (max_iters // 10) == 0 or i == max_iters - 1:\n",
    "        print(f'Iteration {i:2d} | Loss (Mini-Batch): {loss.data:.4f}')\n",
    "\n",
    "# Plot the mini-batch loss\n",
    "plt.plot(losses)\n",
    "\n",
    "print(f'Train loss: {split_loss(\"train\"):.4f} | Val loss: {split_loss(\"val\"):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of the Neuron Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One iteration of mini-batch gradient descent\n",
    "\n",
    "ix = torch.randint(0, x_train.shape[0], (batch_size, )) # Mini-batches\n",
    "xb, yb = x_train[ix], y_train[ix]\n",
    "\n",
    "# Forward pass\n",
    "emb = C[xb] # Embed characters into vectors (B,T) -> (B,T,C)\n",
    "emb = emb.view(emb.size(0), -1)  # Concatenate the embeddings (B,T,C) -> (B,TC)\n",
    "hpreact = emb @ W_hidden + b_hidden\n",
    "h = torch.tanh(hpreact)\n",
    "logits = h @ W_out + b_out\n",
    "\n",
    "# Cross entropy loss\n",
    "loss = F.cross_entropy(logits, yb)\n",
    "losses.append(loss.data)\n",
    "\n",
    "# Backward pass\n",
    "for param in params:\n",
    "    param.grad = None # Set the gradient to zero\n",
    "loss.backward()\n",
    "\n",
    "# Update the parameters\n",
    "for param in params:\n",
    "    param.data -= lr * param.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(14, 5))\n",
    "axs[0].hist(hpreact.view(-1).tolist(), bins=50)\n",
    "axs[0].set_title('Pre-Activation')\n",
    "axs[1].hist(h.view(-1).tolist(), bins=50)\n",
    "axs[1].set_title('Activation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vast majority of activations are either -1 or 1. This means that when back-propagation is performed, the gradients of these activations are 0 and so gradient flow is stopped and the weights further back in the network are not updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.imshow(h.abs() > 0.99, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For any one of the 200 neurons, if an entire column is white, then the neuron is dead. This is where the neuron is not activated by any of the inputs and so the weights are not updated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix: Tanh Layer too Saturated at Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-initialise the parameters with small random values for the hidden layer\n",
    "\n",
    "C = torch.randn((vocab_size, n_embd)) # Embedding table (B,T) -> (B,T,C)\n",
    "# Hidden layer\n",
    "W_hidden = torch.randn((block_size * n_embd, n_hidden)) * 0.2\n",
    "b_hidden = torch.randn(n_hidden) * 0.01\n",
    "# Output layer\n",
    "W_out = torch.randn((n_hidden, vocab_size)) * 0.01 # Initialised to small values\n",
    "b_out = torch.randn(vocab_size) * 0 # Initialised to zero\n",
    "\n",
    "params = [C, W_hidden, b_hidden, W_out, b_out]\n",
    "for param in params:\n",
    "    param.requires_grad = True\n",
    "\n",
    "# One iteration of mini-batch gradient descent\n",
    "\n",
    "ix = torch.randint(0, x_train.shape[0], (batch_size, )) # Mini-batches\n",
    "xb, yb = x_train[ix], y_train[ix]\n",
    "\n",
    "# Forward pass\n",
    "emb = C[xb] # Embed characters into vectors (B,T) -> (B,T,C)\n",
    "emb = emb.view(emb.size(0), -1)  # Concatenate the embeddings (B,T,C) -> (B,TC)\n",
    "hpreact = emb @ W_hidden + b_hidden\n",
    "h = torch.tanh(hpreact)\n",
    "logits = h @ W_out + b_out\n",
    "\n",
    "# Cross entropy loss\n",
    "loss = F.cross_entropy(logits, yb)\n",
    "losses.append(loss.data)\n",
    "\n",
    "# Backward pass\n",
    "for param in params:\n",
    "    param.grad = None # Set the gradient to zero\n",
    "loss.backward()\n",
    "\n",
    "# Update the parameters\n",
    "for param in params:\n",
    "    param.data -= lr * param.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(14, 5))\n",
    "axs[0].hist(hpreact.view(-1).tolist(), bins=50)\n",
    "axs[0].set_title('Pre-Activation')\n",
    "axs[1].hist(h.view(-1).tolist(), bins=50)\n",
    "axs[1].set_title('Activation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.imshow(h.abs() > 0.99, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mini-batch gradient descent\n",
    "losses = []\n",
    "for i in range(max_iters):\n",
    "\n",
    "    ix = torch.randint(0, x_train.shape[0], (batch_size, )) # Mini-batches\n",
    "    xb, yb = x_train[ix], y_train[ix]\n",
    "\n",
    "    # Forward pass\n",
    "    emb = C[xb] # Embed characters into vectors (B,T) -> (B,T,C)\n",
    "    emb = emb.view(emb.size(0), -1)  # Concatenate the embeddings (B,T,C) -> (B,TC)\n",
    "    hpreact = emb @ W_hidden + b_hidden\n",
    "    h = torch.tanh(hpreact)\n",
    "    logits = h @ W_out + b_out\n",
    "\n",
    "    # Cross entropy loss\n",
    "    loss = F.cross_entropy(logits, yb)\n",
    "    losses.append(loss.data)\n",
    "\n",
    "    # Backward pass\n",
    "    for param in params:\n",
    "        param.grad = None # Set the gradient to zero\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the parameters\n",
    "    for param in params:\n",
    "        param.data -= lr * param.grad\n",
    "\n",
    "    if i % (max_iters // 10) == 0 or i == max_iters - 1:\n",
    "        print(f'Iteration {i:2d} | Loss (Mini-Batch): {loss.data:.4f}')\n",
    "\n",
    "# Plot the mini-batch loss\n",
    "plt.plot(losses)\n",
    "\n",
    "print(f'Train loss: {split_loss(\"train\"):.4f} | Val loss: {split_loss(\"val\"):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kaiming Initialisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Kaiming initialisation](https://arxiv.org/abs/1502.01852) is a method of initialising the weights in a neural network such that the variance of the activations is the same across all layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of the need for Kaiming initialisation\n",
    "x = torch.randn(batch_size, block_size * n_embd)\n",
    "w = torch.randn(block_size * n_embd, n_hidden)\n",
    "y = x @ w\n",
    "fig, axs = plt.subplots(1, 2, figsize=(14, 5))\n",
    "axs[0].hist(x.view(-1).tolist(), bins=50, density=True)\n",
    "axs[0].set_title(f'x (mean={x.mean():.2f}, std={x.std():.2f})')\n",
    "axs[1].hist(y.view(-1).tolist(), bins=50, density=True)\n",
    "axs[1].set_title(f'y (mean={y.mean():.2f}, std={y.std():.2f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard deviation of `y` has expanded after performing `y = x @ w`. This deteroriates the performance of the network as the weights should be normalised to prevent the activations from saturating. Kaiming initialisation is used to prevent this by scaling the weights. With a $\\tanh$ activation function, the weights are scaled by $\\frac{5}{3} \\cdot \\frac{1}{\\sqrt{n_{in}}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaiming_init = 1 / (block_size * n_embd) ** 0.5\n",
    "x = torch.randn(batch_size, block_size * n_embd)\n",
    "w = torch.randn(block_size * n_embd, n_hidden) * kaiming_init\n",
    "y = x @ w\n",
    "fig, axs = plt.subplots(1, 2, figsize=(14, 5))\n",
    "axs[0].hist(x.view(-1).tolist(), bins=50, density=True)\n",
    "axs[0].set_title(f'x (mean={x.mean():.2f}, std={x.std():.2f})')\n",
    "axs[1].hist(y.view(-1).tolist(), bins=50, density=True)\n",
    "axs[1].set_title(f'y (mean={y.mean():.2f}, std={y.std():.2f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = torch.randn((vocab_size, n_embd)) # Embedding table (B,T) -> (B,T,C)\n",
    "# Hidden layer\n",
    "kaiming_init = (5/3) / (block_size * n_embd) ** 0.5 # Ensures the variance of the activations is preserved\n",
    "W_hidden = torch.randn((block_size * n_embd, n_hidden)) * kaiming_init\n",
    "b_hidden = torch.randn(n_hidden) * 0.01\n",
    "# Output layer\n",
    "W_out = torch.randn((n_hidden, vocab_size)) * 0.01 # Initialised to small values\n",
    "b_out = torch.randn(vocab_size) * 0 # Initialised to zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Batch normalisation](https://arxiv.org/abs/1502.03167) is used to control the distribution of activations in neural networks. It is common to use batch normalisation layers throughout networks, usually after layers that have multiplications. Batch normalisation has learned parameters `bn_gain` and `bn_bias` controlling the scale and offset of the normalised distributions of the activations. It also has buffers `bn_mean` and `bn_std` which are not trainable via backpropagation and these calculate the mean and the standard deviations of the entire set of inputs. Batch normalisation layers prevent the activations from saturating and allow the network to learn more effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch normalisation params\n",
    "bn_gain = torch.ones((1, n_hidden)) # Scaling for normalised activations\n",
    "bn_bias = torch.zeros((1, n_hidden)) # Offset for normalised activations\n",
    "\n",
    "# Running mean and standard deviation over the training set\n",
    "bn_mean = torch.zeros((1, n_hidden))\n",
    "bn_std = torch.ones((1, n_hidden))\n",
    "\n",
    "params = [C, W_hidden, W_out, b_out, bn_gain, bn_bias]\n",
    "for param in params:\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there is no need to have a bias term in the linear layer `b_hidden` as the batch normalization step inherently adjusts the mean of the output to zero. Thus, the bias term gets subtracted out in the normalization step, making it redundant. The `bn_bias` parameter in the batch normalisation layer can be used to replace the bias term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefine the split_loss function to include batch normalisation\n",
    "\n",
    "@torch.no_grad()\n",
    "def split_loss(split: str) -> None:\n",
    "    \"\"\"Evaluate the loss over the given split.\"\"\"\n",
    "    x, y = (x_train, y_train) if split == 'train' else (x_val, y_val)\n",
    "    # Forward pass\n",
    "    emb = C[x] # Embed characters into vectors (B,T) -> (B,T,C)\n",
    "    emb = emb.view(emb.size(0), -1) # Concatenate the embeddings (B,T,C) -> (B,TC)\n",
    "    hpreact = emb @ W_hidden\n",
    "    # Batch normalisation\n",
    "    hpreact = bn_gain * (hpreact - bn_mean) / bn_std + bn_bias\n",
    "    h = torch.tanh(hpreact)\n",
    "    logits = h @ W_out + b_out\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    return loss.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_lr = 1e-1 # Initial learning rate\n",
    "final_lr = 1e-2 # Final learning rate\n",
    "momentum = 1e-3 # Momentum for the moving average of the gradients\n",
    "\n",
    "# Mini-batch gradient descent\n",
    "losses = []\n",
    "for i in range(max_iters):\n",
    "\n",
    "    ix = torch.randint(0, x_train.shape[0], (batch_size, )) # Mini-batches\n",
    "    xb, yb = x_train[ix], y_train[ix]\n",
    "\n",
    "    # Forward pass\n",
    "    emb = C[xb] # Embed characters into vectors (B,T) -> (B,T,C)\n",
    "    emb = emb.view(emb.size(0), -1)  # Concatenate the embeddings (B,T,C) -> (B,TC)\n",
    "    hpreact = emb @ W_hidden\n",
    "    # Batch normalisation\n",
    "    bn_mean_i = hpreact.mean(0, keepdim=True)\n",
    "    bn_std_i = hpreact.std(0, keepdim=True)\n",
    "    hpreact = bn_gain * (hpreact - bn_mean_i) / bn_std_i + bn_bias # Normalise the activations\n",
    "\n",
    "    # Update the runnning mean and std on the side of the forward pass\n",
    "    with torch.no_grad():\n",
    "        bn_mean = (1 - momentum) * bn_mean + momentum * bn_mean_i\n",
    "        bn_std = (1 - momentum) * bn_std + momentum * bn_std_i\n",
    "        \n",
    "    h = torch.tanh(hpreact)\n",
    "    logits = h @ W_out + b_out\n",
    "\n",
    "    # Cross entropy loss\n",
    "    loss = F.cross_entropy(logits, yb)\n",
    "    losses.append(loss.data)\n",
    "\n",
    "    # Backward pass\n",
    "    for param in params:\n",
    "        param.grad = None # Set the gradient to zero\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the parameters\n",
    "    lr = init_lr if i < (max_iters / 2) else final_lr\n",
    "    for param in params:\n",
    "        param.data -= lr * param.grad\n",
    "\n",
    "    if i % (max_iters // 10) == 0 or i == max_iters - 1:\n",
    "        print(f'Iteration {i:2d} | Loss (Mini-Batch): {loss.data:.4f}')\n",
    "\n",
    "# Plot the mini-batch loss\n",
    "plt.plot(losses)\n",
    "\n",
    "print(f'Train loss: {split_loss(\"train\"):.4f} | Val loss: {split_loss(\"val\"):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    \"\"\"Linear layer with an optional bias.\"\"\"\n",
    "\n",
    "    def __init__(self, n_in: int, n_out: int, bias: bool = True):\n",
    "        self.weights = torch.randn((n_in, n_out)) / n_in ** 0.5 # Kaiming initialisation\n",
    "        self.bias = torch.zeros(n_out) if bias else None\n",
    "\n",
    "    def __call__(self, x: list[float]):\n",
    "        self.out = x @ self.weights\n",
    "        if self.bias is not None:\n",
    "            self.out += self.bias\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.weights] + ([] if self.bias is None else [self.bias])\n",
    "\n",
    "\n",
    "class BatchNorm1D:\n",
    "    \"\"\"Batch normalisation layer.\"\"\"\n",
    "\n",
    "    def __init__(self, n_dims: int, epsilon: float = 1e-5, momentum: float = 1e-1):\n",
    "        self.epsilon = epsilon # Small value to avoid division by zero\n",
    "        self.momentum = momentum # Update rate for the running mean and std\n",
    "        self.training = True\n",
    "        # Parameters\n",
    "        self.gamma = torch.ones(n_dims) # Scaling for normalised activations\n",
    "        self.beta = torch.zeros(n_dims) # Offset for normalised activations\n",
    "        # Buffers\n",
    "        self.mean = torch.zeros(n_dims)\n",
    "        self.var = torch.ones(n_dims)\n",
    "\n",
    "    def __call__(self, x: list[float]):\n",
    "        # Forward pass\n",
    "        if self.training:\n",
    "            x_mean = x.mean(0, keepdim=True) # Batch mean\n",
    "            x_var = x.var(0, keepdim=True) # Batch variance\n",
    "        else:\n",
    "            x_mean = self.mean\n",
    "            x_var = self.var\n",
    "        x_norm = (x - x_mean) / torch.sqrt(x_var + self.epsilon) # Normalise to unit variance\n",
    "        self.out = self.gamma * x_norm + self.beta\n",
    "        # Update buffers\n",
    "        if self.training:\n",
    "            with torch.no_grad():\n",
    "                self.mean = (1 - self.momentum) * self.mean + self.momentum * x_mean\n",
    "                self.var = (1 - self.momentum) * self.var + self.momentum * x_var\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]\n",
    "\n",
    "class Tanh:\n",
    "    \"\"\"Tanh activation function.\"\"\"\n",
    "\n",
    "    def __call__(self, x: list[float]):\n",
    "        self.out = torch.tanh(x)\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = torch.randn((vocab_size, n_embd)) # Embedding matrix\n",
    "\n",
    "# MLP\n",
    "layers = [\n",
    "    Linear(n_embd * block_size, n_hidden, bias=False), BatchNorm1D(n_hidden), Tanh(),\n",
    "    Linear(n_hidden, n_hidden, bias=False), BatchNorm1D(n_hidden), Tanh(),\n",
    "    Linear(n_hidden, n_hidden, bias=False), BatchNorm1D(n_hidden), Tanh(),\n",
    "    Linear(n_hidden, n_hidden, bias=False), BatchNorm1D(n_hidden), Tanh(),\n",
    "    Linear(n_hidden, n_hidden, bias=False), BatchNorm1D(n_hidden), Tanh(),\n",
    "    Linear(n_hidden, vocab_size, bias=False), BatchNorm1D(vocab_size)\n",
    "]\n",
    "\n",
    "# Initialisations\n",
    "with torch.no_grad():\n",
    "    layers[-1].gamma *= 0.1 # Ensure the output layer is not too confident initially\n",
    "    for layer in layers[:-1]: # For all other layers, apply gain in Kaiming initialisation\n",
    "        if isinstance(layer, Linear):\n",
    "            layer.weights *= 5/3\n",
    "\n",
    "params = [C] + [param for layer in layers for param in layer.parameters()]\n",
    "for param in params:\n",
    "    param.requires_grad = True\n",
    "    \n",
    "print(f'Model parameters: {sum(param.nelement() for param in params)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagnostic Plots from 1000 Iterations of Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "updates = [] # Update magnitude for each parameter in each iteration\n",
    "\n",
    "# First 1000 iterations of mini-batch gradient descent\n",
    "for i in range(1000):\n",
    "\n",
    "    ix = torch.randint(0, x_train.shape[0], (batch_size, )) # Mini-batches\n",
    "    xb, yb = x_train[ix], y_train[ix]\n",
    "\n",
    "    # Forward pass\n",
    "    emb = C[xb] # Embed characters into vectors (B,T) -> (B,T,C)\n",
    "    x = emb.view(emb.size(0), -1)  # Concatenate the embeddings (B,T,C) -> (B,TC)\n",
    "    for layer in layers:\n",
    "        x = layer(x)\n",
    "    loss = F.cross_entropy(x, yb) # Cross entropy loss\n",
    "    losses.append(loss.data)\n",
    "\n",
    "    # Backward pass\n",
    "    for layer in layers:\n",
    "        layer.out.retain_grad()\n",
    "    for param in params:\n",
    "        param.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the parameters\n",
    "    lr = init_lr if i < (max_iters / 2) else final_lr\n",
    "    for param in params:\n",
    "        param.data -= lr * param.grad\n",
    "\n",
    "    with torch.no_grad(): # Compute the update magnitude for each parameter\n",
    "        updates.append([((lr * param.grad).std() / param.data.std()).log10().item() for param in params])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise histograms of the activations\n",
    "plt.figure(figsize=(20, 4))\n",
    "legends = []\n",
    "for i, layer in enumerate(layers):\n",
    "    if isinstance(layer, Tanh):\n",
    "        t = layer.out\n",
    "        hy, hx = torch.histogram(t, density=True)\n",
    "        plt.plot(hx[:-1].detach(), hy.detach())\n",
    "        legends.append(f'Layer {i} ({layer.__class__.__name__}) | Mean: {t.mean():.2f} | Std: {t.std():.2f} | Saturation: {((t.abs() > 0.97).float().mean() * 100):.2f}%')\n",
    "plt.legend(legends)\n",
    "plt.title('Activation Distributions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise histograms of the gradients of the activations\n",
    "plt.figure(figsize=(20, 4))\n",
    "legends = []\n",
    "for i, layer in enumerate(layers):\n",
    "    if isinstance(layer, Tanh):\n",
    "        t = layer.out.grad\n",
    "        hy, hx = torch.histogram(t, density=True)\n",
    "        plt.plot(hx[:-1].detach(), hy.detach())\n",
    "        legends.append(f'Layer {i} ({layer.__class__.__name__}) | Mean: {t.mean():.2e} | Std: {t.std():.2e}')\n",
    "plt.legend(legends)\n",
    "plt.title('Activation Distributions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the update magnitude for each parameter in each iteration\n",
    "plt.figure(figsize=(20, 4))\n",
    "legends = []\n",
    "for i, param in enumerate(params):\n",
    "  if param.ndim == 2: # Only plot the weights and embedding matrix\n",
    "    plt.plot([updates[j][i] for j in range(len(updates))])\n",
    "    legends.append(f'Param {i} {tuple(param.shape)}')\n",
    "plt.plot([0, len(updates)], [-3, -3], 'k') # Ratios should be 1e-3\n",
    "plt.legend(legends)\n",
    "plt.title('Update Magnitude for Parameters in Each Iteration')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation on Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefine the split_loss function\n",
    "\n",
    "@torch.no_grad()\n",
    "def split_loss(split: str) -> None:\n",
    "    \"\"\"Evaluate the loss over the given split.\"\"\"\n",
    "    x, y = (x_train, y_train) if split == 'train' else (x_val, y_val)\n",
    "    # Forward pass\n",
    "    emb = C[x] # Embed characters into vectors (B,T) -> (B,T,C)\n",
    "    x = emb.view(emb.size(0), -1) # Concatenate the embeddings (B,T,C) -> (B,TC)\n",
    "    for layer in layers:\n",
    "        x = layer(x)\n",
    "    loss = F.cross_entropy(x, y)\n",
    "    return loss.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = torch.randn((vocab_size, n_embd)) # Embedding matrix\n",
    "\n",
    "# MLP\n",
    "layers = [\n",
    "    Linear(n_embd * block_size, n_hidden, bias=False), BatchNorm1D(n_hidden), Tanh(),\n",
    "    Linear(n_hidden, n_hidden, bias=False), BatchNorm1D(n_hidden), Tanh(),\n",
    "    Linear(n_hidden, n_hidden, bias=False), BatchNorm1D(n_hidden), Tanh(),\n",
    "    Linear(n_hidden, n_hidden, bias=False), BatchNorm1D(n_hidden), Tanh(),\n",
    "    Linear(n_hidden, n_hidden, bias=False), BatchNorm1D(n_hidden), Tanh(),\n",
    "    Linear(n_hidden, vocab_size, bias=False), BatchNorm1D(vocab_size)\n",
    "]\n",
    "\n",
    "# Initialisations\n",
    "with torch.no_grad():\n",
    "    layers[-1].gamma *= 0.1 # Ensure the output layer is not too confident initially\n",
    "    for layer in layers[:-1]: # For all other layers, apply gain in Kaiming initialisation\n",
    "        if isinstance(layer, Linear):\n",
    "            layer.weights *= 5/3\n",
    "\n",
    "params = [C] + [param for layer in layers for param in layer.parameters()]\n",
    "for param in params:\n",
    "    param.requires_grad = True\n",
    "    \n",
    "print(f'Model parameters: {sum(param.nelement() for param in params)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mini-batch gradient descent\n",
    "losses = []\n",
    "for i in range(max_iters):\n",
    "\n",
    "    ix = torch.randint(0, x_train.shape[0], (batch_size, )) # Mini-batches\n",
    "    xb, yb = x_train[ix], y_train[ix]\n",
    "\n",
    "    # Forward pass\n",
    "    emb = C[xb] # Embed characters into vectors (B,T) -> (B,T,C)\n",
    "    x = emb.view(emb.size(0), -1)  # Concatenate the embeddings (B,T,C) -> (B,TC)\n",
    "    for layer in layers:\n",
    "        x = layer(x)\n",
    "    loss = F.cross_entropy(x, yb) # Cross entropy loss\n",
    "    losses.append(loss.data)\n",
    "\n",
    "    # Backward pass\n",
    "    for param in params:\n",
    "        param.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the parameters\n",
    "    lr = init_lr if i < (max_iters / 2) else final_lr\n",
    "    for param in params:\n",
    "        param.data -= lr * param.grad\n",
    "\n",
    "    with torch.no_grad(): # Compute the update magnitude for each parameter\n",
    "        updates.append([((lr * param.grad).std() / param.data.std()).log10().item() for param in params])\n",
    "\n",
    "\n",
    "    if i % (max_iters // 10) == 0 or i == max_iters - 1:\n",
    "        print(f'Iteration {i:2d} | Loss (Mini-Batch): {loss.data:.4f}')\n",
    "\n",
    "# Plot the mini-batch loss\n",
    "plt.plot(losses)\n",
    "\n",
    "for layer in layers: # Disable batch normalisation during evaluation\n",
    "  layer.training = False\n",
    "print(f'Train loss: {split_loss(\"train\"):.4f} | Val loss: {split_loss(\"val\"):.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from the model\n",
    "for _ in range(5):\n",
    "    out = []\n",
    "    context = [0] * block_size # Initialise context to '...'\n",
    "\n",
    "    while True:\n",
    "        # Forward pass\n",
    "        emb = C[torch.tensor([context])] # Embed characters into vectors (B,T) -> (B,T,C)\n",
    "        x = emb.view(emb.size(0), -1) # Concatenate the embeddings (B,T,C) -> (B,TC)\n",
    "        for layer in layers:\n",
    "            x = layer(x)\n",
    "        probs = F.softmax(x, dim=1)\n",
    "\n",
    "        # Sample the next character from the distribution for the current character index\n",
    "        ix = torch.multinomial(probs, num_samples=1, replacement=True).item()\n",
    "        context = context[1:] + [ix] # Shift the context window\n",
    "        out.append(int_to_char[ix])\n",
    "        if ix == 0:\n",
    "            break # End of word\n",
    "    print(''.join(out))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
