{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model in this notebook is based on a simplified version of the [WaveNet](https://arxiv.org/abs/1609.03499) architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('names.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read().split('\\n')\n",
    "\n",
    "# Unique characters in the text\n",
    "chars = ['.'] + sorted(list(set(''.join(text))))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# Mapping from characters to integers and vice versa\n",
    "char_to_int = {c: i for i, c in enumerate(chars)}\n",
    "int_to_char = {i: c for i, c in enumerate(chars)}\n",
    "\n",
    "# B - batch size, T - block size, C - embedding dimension (vocab size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 8 # Context length for predictions\n",
    "\n",
    "def build_dataset(text: list[str]) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Builds the dataset for training the model.\"\"\"\n",
    "    X, Y = [], []\n",
    "    for word in text:\n",
    "        context = [0] * block_size # Padding the context with initial '.' characters\n",
    "        for char in word + '.':\n",
    "            ix = char_to_int[char]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix] # Update context\n",
    "    return torch.tensor(X), torch.tensor(Y)\n",
    "\n",
    "random.shuffle(text)\n",
    "split = int(len(text) * 0.9) # 90% train, 10% val\n",
    "\n",
    "x_train, y_train = build_dataset(text[:split])\n",
    "x_val, y_val = build_dataset(text[split:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WaveNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    \"\"\"Linear layer with an optional bias.\"\"\"\n",
    "\n",
    "    def __init__(self, n_in: int, n_out: int, bias: bool = True):\n",
    "        self.weights = torch.randn((n_in, n_out)) / n_in ** 0.5 # Kaiming initialisation\n",
    "        self.bias = torch.zeros(n_out) if bias else None\n",
    "\n",
    "    def __call__(self, x: list[float]):\n",
    "        self.out = x @ self.weights\n",
    "        if self.bias is not None:\n",
    "            self.out += self.bias\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.weights] + ([] if self.bias is None else [self.bias])\n",
    "\n",
    "\n",
    "class BatchNorm1D:\n",
    "    \"\"\"Batch normalisation layer.\"\"\"\n",
    "\n",
    "    def __init__(self, n_dims: int, epsilon: float = 1e-5, momentum: float = 1e-1):\n",
    "        self.epsilon = epsilon # Small value to avoid division by zero\n",
    "        self.momentum = momentum # Update rate for the running mean and std\n",
    "        self.training = True\n",
    "        # Parameters\n",
    "        self.gamma = torch.ones(n_dims) # Scaling for normalised activations\n",
    "        self.beta = torch.zeros(n_dims) # Offset for normalised activations\n",
    "        # Buffers\n",
    "        self.mean = torch.zeros(n_dims)\n",
    "        self.var = torch.ones(n_dims)\n",
    "\n",
    "    def __call__(self, x: list[float]):\n",
    "        # Forward pass\n",
    "        if self.training:\n",
    "            if x.ndim == 2:\n",
    "                dim = 0\n",
    "            elif x.ndim == 3:\n",
    "                dim = (0, 1)\n",
    "            x_mean = x.mean(dim, keepdim=True) # Batch mean\n",
    "            x_var = x.var(dim, keepdim=True) # Batch variance\n",
    "        else:\n",
    "            x_mean = self.mean\n",
    "            x_var = self.var\n",
    "        x_norm = (x - x_mean) / torch.sqrt(x_var + self.epsilon) # Normalise to unit variance\n",
    "        self.out = self.gamma * x_norm + self.beta\n",
    "        # Update buffers\n",
    "        if self.training:\n",
    "            with torch.no_grad():\n",
    "                self.mean = (1 - self.momentum) * self.mean + self.momentum * x_mean\n",
    "                self.var = (1 - self.momentum) * self.var + self.momentum * x_var\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]\n",
    "\n",
    "class Tanh:\n",
    "    \"\"\"Tanh activation function.\"\"\"\n",
    "\n",
    "    def __call__(self, x: list[float]):\n",
    "        self.out = torch.tanh(x)\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "class Embedding:\n",
    "    \"\"\"Embedding layer.\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd: int, n_dims: int):\n",
    "        self.weights = torch.randn((n_embd, n_dims))\n",
    "\n",
    "    def __call__(self, x: list[float]):\n",
    "        self.out = self.weights[x] # Embed characters into vectors (B,T) -> (B,T,C)\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.weights]\n",
    "    \n",
    "class FlattenConsecutive:\n",
    "    \"\"\"Flattens consecutive elements.\"\"\"\n",
    "\n",
    "    def __init__(self, n):\n",
    "        self.n = n\n",
    "\n",
    "    def __call__(self, x: list[float]):\n",
    "        x = x.view(x.size(0), x.size(1) // self.n, x.size(2) * self.n) # Concatenate the embeddings (B,T,C) -> (B,T//n,C*n)\n",
    "        if x.size(1) == 1: # Remove T dimension if equal to 1\n",
    "            x = x.squeeze(1)\n",
    "        self.out = x\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "class Sequential:\n",
    "    \"\"\"Sequential model.\"\"\"\n",
    "\n",
    "    def __init__(self, layers: list):\n",
    "        self.layers = layers\n",
    "\n",
    "    def __call__(self, x: list[float]):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        self.out = x\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [param for layer in self.layers for param in layer.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embd = 24 # Embedding dimension\n",
    "n_hidden = 128 # Neurons in the hidden layer\n",
    "\n",
    "# WaveNet\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size, n_embd),\n",
    "    FlattenConsecutive(2), Linear(n_embd * 2, n_hidden, bias=False), BatchNorm1D(n_hidden), Tanh(),\n",
    "    FlattenConsecutive(2), Linear(n_hidden * 2, n_hidden, bias=False), BatchNorm1D(n_hidden), Tanh(),\n",
    "    FlattenConsecutive(2), Linear(n_hidden * 2, n_hidden, bias=False), BatchNorm1D(n_hidden), Tanh(),\n",
    "    Linear(n_hidden, vocab_size, bias=False)\n",
    "])\n",
    "\n",
    "# Initialisations\n",
    "with torch.no_grad():\n",
    "    model.layers[-1].weights *= 0.1 # Ensure the output layer is not too confident initially\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "print(f'Model parameters: {sum(param.nelement() for param in model.parameters())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def split_loss(split: str) -> None:\n",
    "    \"\"\"Evaluate the loss over the given split.\"\"\"\n",
    "    x, y = (x_train, y_train) if split == 'train' else (x_val, y_val)\n",
    "    # Forward pass\n",
    "    logits = model(x)\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    return loss.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward a batch of 4 examples for layer by layer inspection\n",
    "ix = torch.randint(0, x_train.shape[0], (4, ))\n",
    "xb, yb = x_train[ix], y_train[ix]\n",
    "logits = model(xb)\n",
    "\n",
    "for layer in model.layers:\n",
    "    print(f'{layer.__class__.__name__}: {tuple(layer.out.size())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_lr = 1e-1 # Initial learning rate\n",
    "final_lr = 1e-2 # Final learning rate\n",
    "lr_switch = 3 / 4 # Ratio of iterations to switch from initial to final learning rate\n",
    "batch_size = 32 # Number of samples per batch\n",
    "max_iters = 100000\n",
    "\n",
    "# Mini-batch gradient descent\n",
    "losses = []\n",
    "for i in range(max_iters):\n",
    "\n",
    "    ix = torch.randint(0, x_train.shape[0], (batch_size, )) # Mini-batches\n",
    "    xb, yb = x_train[ix], y_train[ix]\n",
    "\n",
    "    # Forward pass\n",
    "    logits = model(xb)\n",
    "    loss = F.cross_entropy(logits, yb) # Cross entropy loss\n",
    "    losses.append(loss.data)\n",
    "\n",
    "    # Backward pass\n",
    "    for param in model.parameters():\n",
    "        param.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the parameters\n",
    "    lr = init_lr if i < (max_iters * lr_switch) else final_lr\n",
    "    for param in model.parameters():\n",
    "        param.data -= lr * param.grad\n",
    "\n",
    "    if i % (max_iters // 10) == 0 or i == max_iters - 1:\n",
    "        print(f'Iteration {i:2d} | Loss (Mini-Batch): {loss.data:.4f}')\n",
    "\n",
    "# Plot the loss, averaged over 100 iterations\n",
    "plt.plot(torch.tensor(losses).view(-1, max_iters // 100).mean(dim=1))\n",
    "\n",
    "for layer in model.layers: # Disable batch normalisation during evaluation\n",
    "  layer.training = False\n",
    "print(f'Train loss: {split_loss(\"train\"):.4f} | Val loss: {split_loss(\"val\"):.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from the model\n",
    "for _ in range(5):\n",
    "    out = []\n",
    "    context = [0] * block_size # Initialise context to '...'\n",
    "\n",
    "    while True:\n",
    "        # Forward pass\n",
    "        logits = model(torch.tensor([context]))\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        # Sample the next character from the distribution for the current character index\n",
    "        ix = torch.multinomial(probs, num_samples=1, replacement=True).item()\n",
    "        context = context[1:] + [ix] # Shift the context window\n",
    "        out.append(int_to_char[ix])\n",
    "        if ix == 0:\n",
    "            break # End of word\n",
    "    print(''.join(out))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
