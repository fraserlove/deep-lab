{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('names.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read().split('\\n')\n",
    "\n",
    "# Unique characters in the text\n",
    "chars = ['.'] + sorted(list(set(''.join(text))))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# Mapping from characters to integers and vice versa\n",
    "char_to_int = {c: i for i, c in enumerate(chars)}\n",
    "int_to_char = {i: c for i, c in enumerate(chars)}\n",
    "\n",
    "# B - batch size, T - block size, C - embedding dimension (vocab size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 3 # Context length for predictions\n",
    "\n",
    "def build_dataset(text: list[str]) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Builds the dataset for training the model.\"\"\"\n",
    "    X, Y = [], []\n",
    "    for word in text:\n",
    "        context = [0] * block_size # Padding the context with initial '.' characters\n",
    "        for char in word + '.':\n",
    "            ix = char_to_int[char]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix] # Update context\n",
    "    return torch.tensor(X), torch.tensor(Y)\n",
    "\n",
    "random.shuffle(text)\n",
    "split = int(len(text) * 0.9) # 90% train, 10% val\n",
    "\n",
    "x_train, y_train = build_dataset(text[:split])\n",
    "x_val, y_val = build_dataset(text[split:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embd = 2 # Embedding dimension\n",
    "n_hidden = 200 # Neurons in the hidden layer\n",
    "\n",
    "C = torch.randn((vocab_size, n_embd)) # Embedding table (B,T) -> (B,T,C)\n",
    "# Hidden layer\n",
    "kaiming_init = (5/3) / (block_size * n_embd) ** 0.5\n",
    "W_hidden = torch.randn((block_size * n_embd, n_hidden)) * kaiming_init\n",
    "# Output layer\n",
    "W_out = torch.randn((n_hidden, vocab_size)) * 0.01\n",
    "b_out = torch.randn(vocab_size) * 0\n",
    "\n",
    "# Batch normalisation params\n",
    "bn_gain = torch.ones((1, n_hidden))\n",
    "bn_bias = torch.zeros((1, n_hidden))\n",
    "bn_mean_live = torch.zeros((1, n_hidden))\n",
    "bn_std_live = torch.ones((1, n_hidden))\n",
    "\n",
    "params = [C, W_hidden, W_out, b_out, bn_gain, bn_bias]\n",
    "for param in params:\n",
    "    param.requires_grad = True\n",
    "\n",
    "print(f'Model parameters: {sum(param.nelement() for param in params)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding a Suitable Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128 # Number of samples per batch\n",
    "max_iters = 1000\n",
    "\n",
    "lrs_exp = torch.linspace(-3, 0, max_iters) # Linearly decrease the learning rate from 1e-3 to 1e-0\n",
    "lrs = 10 ** lrs_exp\n",
    "\n",
    "# Mini-batch gradient descent\n",
    "losses = []\n",
    "for i in range(max_iters):\n",
    "\n",
    "    ix = torch.randint(0, x_train.shape[0], (batch_size, )) # Mini-batches\n",
    "    xb, yb = x_train[ix], y_train[ix]\n",
    "\n",
    "    # Forward pass\n",
    "    emb = C[xb] # Embed characters into vectors (B,T) -> (B,T,C)\n",
    "    emb = emb.view(emb.size(0), -1)  # Concatenate the embeddings (B,T,C) -> (B,TC)\n",
    "    h_pre_act = emb @ W_hidden\n",
    "\n",
    "    # Batch normalisation\n",
    "    bn_mean_i = h_pre_act.mean(0, keepdim=True)\n",
    "    bn_std_i = h_pre_act.std(0, keepdim=True)\n",
    "    h_pre_act = bn_gain * (h_pre_act - bn_mean_i) / bn_std_i + bn_bias\n",
    "    with torch.no_grad():\n",
    "        bn_mean_live = 0.999 * bn_mean_live + 0.001 * bn_mean_i\n",
    "        bn_std_live = 0.999 * bn_std_live + 0.001 * bn_std_i\n",
    "        \n",
    "    h = torch.tanh(h_pre_act)\n",
    "    logits = h @ W_out + b_out\n",
    "\n",
    "    # Cross entropy loss\n",
    "    loss = F.cross_entropy(logits, yb)\n",
    "    losses.append(loss.data)\n",
    "\n",
    "    # Backward pass\n",
    "    for param in params:\n",
    "        param.grad = None # Set the gradient to zero\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the parameters\n",
    "    for param in params:\n",
    "        param.data -= lrs[i] * param.grad\n",
    "\n",
    "# Plot the mini-batch loss vs. learning rate\n",
    "plt.plot(lrs_exp, losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data shows the best learning rate occurs at roughly 1e-1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model with a Split Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_lr = 1e-1 # Initial learning rate\n",
    "final_lr = 1e-2 # Final learning rate\n",
    "max_iters = 50000\n",
    "\n",
    "# Mini-batch gradient descent\n",
    "losses = []\n",
    "for i in range(max_iters):\n",
    "\n",
    "    ix = torch.randint(0, x_train.shape[0], (batch_size, )) # Mini-batches\n",
    "    xb, yb = x_train[ix], y_train[ix]\n",
    "\n",
    "    # Forward pass\n",
    "    emb = C[xb] # Embed characters into vectors (B,T) -> (B,T,C)\n",
    "    emb = emb.view(emb.size(0), -1)  # Concatenate the embeddings (B,T,C) -> (B,TC)\n",
    "    h_pre_act = emb @ W_hidden\n",
    "\n",
    "    # Batch normalisation\n",
    "    bn_mean_i = h_pre_act.mean(0, keepdim=True)\n",
    "    bn_std_i = h_pre_act.std(0, keepdim=True)\n",
    "    h_pre_act = bn_gain * (h_pre_act - bn_mean_i) / bn_std_i + bn_bias\n",
    "    with torch.no_grad():\n",
    "        bn_mean_live = 0.999 * bn_mean_live + 0.001 * bn_mean_i\n",
    "        bn_std_live = 0.999 * bn_std_live + 0.001 * bn_std_i\n",
    "        \n",
    "    h = torch.tanh(h_pre_act)\n",
    "    logits = h @ W_out + b_out\n",
    "\n",
    "    # Cross entropy loss\n",
    "    loss = F.cross_entropy(logits, yb)\n",
    "    losses.append(loss.data)\n",
    "\n",
    "    # Backward pass\n",
    "    for param in params:\n",
    "        param.grad = None # Set the gradient to zero\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the parameters\n",
    "    lr = init_lr if i < (max_iters / 2) else final_lr\n",
    "    for param in params:\n",
    "        param.data -= lr * param.grad\n",
    "\n",
    "    if i % (max_iters // 10) == 0 or i == max_iters - 1:\n",
    "        print(f'Iteration {i:2d} | Loss (Mini-Batch): {loss.data:.4f}')\n",
    "\n",
    "# Plot the mini-batch loss\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Model on the Entire Training and Validation Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def split_loss(split: str) -> None:\n",
    "    \"\"\"Evaluate the loss for the given split.\"\"\"\n",
    "    x, y = (x_train, y_train) if split == 'train' else (x_val, y_val)\n",
    "    # Forward pass\n",
    "    emb = C[x] # Embed characters into vectors (B,T) -> (B,T,C)\n",
    "    emb = emb.view(emb.size(0), -1) # Concatenate the embeddings (B,T,C) -> (B,TC)\n",
    "    h_pre_act = emb @ W_hidden\n",
    "    # Batch normalisation\n",
    "    h_pre_act = bn_gain * (h_pre_act - bn_mean_live) / bn_std_live + bn_bias\n",
    "    h = torch.tanh(h_pre_act)\n",
    "    logits = h @ W_out + b_out\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    print(f'{split.capitalize()} Loss: {loss.data}')\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from the model\n",
    "for _ in range(5):\n",
    "    out = []\n",
    "    context = [0] * block_size # Initialise context to '...'\n",
    "\n",
    "    while True:\n",
    "        # Forward pass\n",
    "        emb = C[torch.tensor([context])] # Embed characters into vectors\n",
    "        emb = emb.view(emb.size(0), -1) # Concatenate the embeddings\n",
    "        h_pre_act = emb @ W_hidden\n",
    "        # Batch normalisation\n",
    "        h_pre_act = bn_gain * (h_pre_act - bn_mean_live) / bn_std_live + bn_bias\n",
    "        h = torch.tanh(h_pre_act)\n",
    "        logits = h @ W_out + b_out\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "\n",
    "        # Sample the next character from the distribution for the current character index\n",
    "        ix = torch.multinomial(probs, num_samples=1, replacement=True).item()\n",
    "        context = context[1:] + [ix] # Shift the context window\n",
    "        out.append(int_to_char[ix])\n",
    "        if ix == 0:\n",
    "            break # End of word\n",
    "    print(''.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the embedding matrix C for all characters\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.scatter(C[:,0].data, C[:,1].data, s=200)\n",
    "for i in range(C.shape[0]):\n",
    "    plt.text(C[i,0].item(), C[i,1].item(), int_to_char[i], ha='center', va='center', color='white')\n",
    "plt.grid('minor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has learned to a basic clustering of the characters (all the vowels are clustered together with similar vector embeddings). The embedding dimension was set to 2 for visualisation purposes. A higher embedding dimension (`n_embd = 10`) would likely improve the performance of the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
