# GPT Tokeniser

An implementation of the GPT tokeniser, which is used to convert text into a sequence of tokens that can be fed into a GPT model. This tokeniser is based on the tokeniser used in the [GPT-2](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) and follows the Byte Pair Encoding (BPE) algorithm as given in [minbpe](https://github.com/karpathy/minbpe/tree/master) by Andrej Karpathy.

Two tokenisers are implemented. `BasicTokeniser` is a basic BPE tokeniser that does not handle any special tokens or the regular expression splitting pattern and can be found in `basic.py`. `GPTTokeniser` is a more advanced tokeniser that can handle special tokens and the regular expression splitting pattern and can be found in `gpt.py`. Both tokenisers have the ability to save and load a tokeniser model from `.tkn` files.