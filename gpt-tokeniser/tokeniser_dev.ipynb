{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT Tokeniser Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A token is a sequence of characters in a text that serves as a unit. Furthermore, tokenisation is the process of converting a text into a sequence of tokens. Tokenisation is critical to the correct functioning of transformers and bad tokenisation can cause issues with the models performance irrespective of the model architecture. If tokenisation is not done correctly, transformer models can struggle to spell words, struggle with non-English words, struggle with simple arithmetic, and even produces unintended outputs (see [SolidGoldMagikarp](https://www.lesswrong.com/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation)).\n",
    "\n",
    "Recall the GPT model in `gpt.py`:\n",
    "```Python\n",
    "class GPTLanguageModel(nn.Module):\n",
    "    \"\"\"GPT Decoder model. Consists of an embedding layer, transformer blocks, and a linear head.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embed_table = nn.Embedding(vocab_size, n_embed) # (B,T) -> (B,T,C)\n",
    "        # etc.\n",
    "```\n",
    "Tokens are the fundamental 'atoms' at the input of transformers. Each token (character) is used as an index to look up the corresponding row in the embedding table, where this row is a trainable vector (of size `n_embed`) representation of the token. Using characters as tokens is a naive approach due to the transformers having a limited context window (1024 tokens for GPT-2) in which tokens can attend to each other. Chunk vocabularies are used to tokenise text into character chunks instead of individual characters. These chunk vocabularies are constructed using the Byte Pair Encoding (BPE) algorithm (popularised in the [GPT-2 paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)). Using character chunks as tokens allows the model to attend to a wider portion of the text, which can improve performance.\n",
    "\n",
    "Note that the tokeniser is completely separate from the transformer model. It has a separate training dataset to train the vocabulary on the BPE algorithm. The tokeniser then encodes/decodes between text and sequences of tokens. The transformer model only sees the tokens and never directly deals with any text.\n",
    "\n",
    "[Tiktokeniser](https://tiktokenizer.vercel.app/) provides a visualisation into differences between various tokenisers available for GPT models. Use 'gpt2' and 'cl100k_base' as the model names to compare the tokenisation of GPT-2 and GPT-4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Hello 你好'\n",
    "\n",
    "print([ord(x) for x in text]) # Encoding to unicode values\n",
    "print(list(text.encode('utf-8'))) # Encoding to utf-8 bytes\n",
    "\n",
    "# The utf-8 encoding is different from the unicode values for non-ASCII characters \n",
    "# as it uses a variable number of bytes. 20320 is encoded as 228 189 160 in utf-8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consecutive_pairs(ints: list[int]) -> dict[tuple[int, int], int]:\n",
    "    \"\"\"\n",
    "    Generate a dictionary of the frequencies of consecutive integers in the list.\n",
    "    Example: [1, 2, 3, 1, 2] -> {(1, 2): 2, (2, 3): 1, (3, 1): 1}\n",
    "    \"\"\"\n",
    "    freq = {}\n",
    "    for pair in zip(ints, ints[1:]):\n",
    "        freq[pair] = freq.get(pair, 0) + 1\n",
    "    return freq\n",
    "\n",
    "text = 'abcab'\n",
    "tokens = list(text.encode('utf-8'))\n",
    "\n",
    "freq_pairs = consecutive_pairs(tokens)\n",
    "print(freq_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_pair(ints: list[int], pair: tuple[int, int], new_int: int) -> list[int]:\n",
    "    \"\"\"\n",
    "    Replace all consecutive occurrences of a pair of integers in the list with a new integer.\n",
    "    Example: ints=[1, 2, 3, 1, 2], pair=(1, 2), new_int=4 -> [4, 3, 4]\n",
    "    \"\"\"\n",
    "    new_ints = []\n",
    "    i = 0\n",
    "    while i < len(ints):\n",
    "        # If not at the last position AND the pair matches, replace it\n",
    "        if (i < len(ints) - 1) and ints[i:i+2] == list(pair):\n",
    "            new_ints.append(new_int)\n",
    "            i += 2\n",
    "        else:\n",
    "            new_ints.append(ints[i])\n",
    "            i += 1\n",
    "    return new_ints\n",
    "\n",
    "# Replace the most frequent pair with a new token (256)\n",
    "max_pair = max(freq_pairs, key=freq_pairs.get)\n",
    "new_tokens = replace_pair(tokens, pair=max_pair, new_int=256)\n",
    "print(f'{tokens} -> {new_tokens}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training via Byte Pair Encoding (BPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load new text from a file.\n",
    "with open('test.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "\n",
    "vocab_size = 265 # Desired vocabulary size\n",
    "\n",
    "assert vocab_size >= 256\n",
    "n_merges = vocab_size - 256\n",
    "tokens = list(text.encode('utf-8'))\n",
    "merges = {} # Dictionary to store the merges\n",
    "vocab = {i: bytes([i]) for i in range(256)}\n",
    "\n",
    "# Merge the most frequent pair n_merges times to create new tokens\n",
    "for i in range(n_merges):\n",
    "    # Find the most frequent consecutive pair of tokens\n",
    "    freq_pairs = consecutive_pairs(tokens)\n",
    "    max_pair = max(freq_pairs, key=freq_pairs.get)\n",
    "    # Create a new token and assign it to an unused integer\n",
    "    new_token = 256 + i\n",
    "    tokens = replace_pair(tokens, max_pair, new_token)\n",
    "    # Store the merge and the new token in the vocab\n",
    "    merges[max_pair] = new_token\n",
    "    vocab[new_token] = vocab[max_pair[0]] + vocab[max_pair[1]]\n",
    "    print(f'{i+1}/{n_merges}: {max_pair} -> {new_token}')\n",
    "\n",
    "print(vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The text is now represented by fewer tokens\n",
    "print('New token length:', len(tokens))\n",
    "print(f'Compression ratio: {len(list(text.encode(\"utf-8\"))) / len(tokens):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding and Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(tokens: list[int]) -> str:\n",
    "    \"\"\"Decode a sequence of tokens into a string.\"\"\"\n",
    "    bytes_ = b''.join(vocab[token] for token in tokens)\n",
    "    text = bytes_.decode('utf-8', errors='replace') # Replace unknown characters\n",
    "    return text\n",
    "\n",
    "# Not every byte sequence is valid utf-8. Replacing unknown characters with '?'\n",
    "# helps to avoid decoding errors as the language model may generate tokens that\n",
    "# are not valid utf-8. For example 128 is not a valid utf-8 byte.\n",
    "print(decode([128]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text: str) -> list[int]:\n",
    "    \"\"\"Encode a string into a sequence of tokens.\"\"\"\n",
    "    tokens = list(text.encode('utf-8'))\n",
    "    while len(tokens) > 1:\n",
    "        freq_pairs = consecutive_pairs(tokens)\n",
    "        # Find the most frequent consecutive pair that has been merged\n",
    "        most_freq = min(freq_pairs, key=lambda pair: merges.get(pair, float('inf')))\n",
    "        if most_freq not in merges:\n",
    "            break # No more merges to apply\n",
    "        # Merge the pair into a new token\n",
    "        new_token = merges[most_freq]\n",
    "        tokens = replace_pair(tokens, most_freq, new_token)\n",
    "    return tokens\n",
    "\n",
    "print(encode('the quick brown fox'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the encode and decode functions are inverses\n",
    "text == decode(encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenisation Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokeniser is represented using just the learned `merges` and `vocab` variables and can encode and decode text using the BPE algorithm.\n",
    "\n",
    "**Splitting Text via RegEx Patterns (GPT-2)**\n",
    "\n",
    "Instead of directly encoding each string for tokenisation, the string is split up into a list of strings using regular expressions. All the string in this list are processed independently by the tokeniser. Therefore merges can only happen within the same string. The results are then concatenated together to form the final tokenised string. This ensures that some consecutive pairs of characters are not merged together (i.e. 'e ').\n",
    "\n",
    "The regular expression below is from the [GPT-2 tokeniser](https://github.com/openai/gpt-2/blob/master/src/encoder.py). The patterns `'s|'t|'re|'ve|'m|'ll|'d` match common contractions, however it only considers the ASCII apostrophe (') and not the unicode apostrophe (’). Furthermore, they do not ignore case and so will not match `'S|'T|'RE|'VE|'M|'LL|'D`. That is, `I'm` will be tokenised as `I`, `'m` and `I'M` will be tokenised as `I`, `'`, `M`. These are limitations of the GPT-2 tokeniser.\n",
    "\n",
    "GPT-2 also uses one special token which denotes the end of text, `<|endoftext|>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "\n",
    "regex = re.compile(r\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\")\n",
    "print(re.findall(regex, 'I\\'m. I\\'M'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vocabulary Size**\n",
    "\n",
    "Large vocabulary sizes increase the number of tokens that the model can represent. Therefore, tokens can express more information in a shorter sequence. This allows transformers to attend to more tokens in the sequence and improves the model's ability to learn long-range dependencies. However, larger vocabulary sizes mean that the embedding table is larger and hence more computationally expensive to train. Furthermore, large vocabulary sizes mean that each unique token is less likely to be seen in the training data, hence the vector representation of the token may be under-trained, leading to worse overall performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
